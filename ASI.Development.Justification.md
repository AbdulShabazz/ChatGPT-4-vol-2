## OpenAI ChatGPT v1.2025.238 

**Q: Narrow Artificial General Intelligence in any field is useful enough to advance society along its timeline of innovation, but is there a legitimate reason for a world military or market or business to build Artificial Super Intelligence?**

**A:** Short answer: pursue Narrow-AGI broadly; pursue ASI only under strict, verifiable safeguards that reduce systemic and strategic risk to a socially acceptable level. Without those safeguards, the marginal benefits of ASI do not justify the risk profile.

## Definitions

* Narrow-AGI: systems at or above expert human performance across many tasks within one sector or domain bundle.
* ASI: systems that robustly exceed the best human performance across most economically or strategically relevant tasks, including capability to create, improve, and autonomously deploy novel tools and strategies.

## Why any actor would try to build ASI

1. R\&D acceleration and scientific discovery: frontier AI has already produced new science and algorithms; stronger systems plausibly multiply this. AlphaFold achieved near-atomic accuracy at proteome scale, unlocking previously intractable protein structures. AlphaTensor and AlphaDev discovered faster algorithms for matrix multiplication and sorting, with the latter merged into LLVM. ([Nature][1], [PubMed][2], [Google DeepMind][3])
   Textual sources:

* Nature, “Highly accurate protein structure prediction with AlphaFold”: [https://www.nature.com/articles/s41586-021-03819-2](https://www.nature.com/articles/s41586-021-03819-2)
* Nature, “Highly accurate protein structure prediction for the human proteome”: [https://www.nature.com/articles/s41586-021-03828-1](https://www.nature.com/articles/s41586-021-03828-1)
* Nature, “Discovering faster matrix multiplication algorithms with AlphaTensor”: [https://www.nature.com/articles/s41586-022-05172-4](https://www.nature.com/articles/s41586-022-05172-4)
* Nature, “Faster sorting algorithms discovered using deep reinforcement learning”: [https://www.nature.com/articles/s41586-023-06004-9](https://www.nature.com/articles/s41586-023-06004-9)
* DeepMind blog, “AlphaDev discovers faster sorting algorithms”: [https://deepmind.google/discover/blog/alphadev-discovers-faster-sorting-algorithms/](https://deepmind.google/discover/blog/alphadev-discovers-faster-sorting-algorithms/)

2. Economic value and productivity: firm-level deployments of generative AI show double-digit productivity gains today; macro models project multi-trillion-dollar annual impact as capability improves. ([NBER][4], [McKinsey & Company][5])
   Textual sources:

* NBER Working Paper w31161, “Generative AI at Work”: [https://www.nber.org/papers/w31161](https://www.nber.org/papers/w31161)
* McKinsey, “The economic potential of generative AI”: [https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier) and PDF [https://www.mckinsey.com/\~/media/mckinsey/business%20functions/mckinsey%20digital/our%20insights/the%20economic%20potential%20of%20generative%20ai%20the%20next%20productivity%20frontier/the-economic-potential-of-generative-ai-the-next-productivity-frontier.pdf](https://www.mckinsey.com/~/media/mckinsey/business%20functions/mckinsey%20digital/our%20insights/the%20economic%20potential%20of%20generative%20ai%20the%20next%20productivity%20frontier/the-economic-potential-of-generative-ai-the-next-productivity-frontier.pdf)

3. Strategic and military advantage: autonomy and accelerated OODA loops shift deterrence and battlefield economics; recent swarming deployments illustrate operational leverage and arms-race pressure. ([The Wall Street Journal][6], [Defense Intelligence University][7], [DefenseScoop][8])
   Textual sources:

* WSJ, “AI-Powered Drone Swarms Have Now Entered the Battlefield”: [https://www.wsj.com/world/ai-powered-drone-swarms-have-now-entered-the-battlefield-2cab0f05](https://www.wsj.com/world/ai-powered-drone-swarms-have-now-entered-the-battlefield-2cab0f05)
* DIU, “The Replicator initiative”: [https://www.diu.mil/replicator](https://www.diu.mil/replicator)
* DefenseScoop, “DoD touts ‘successful transition’ for Replicator initiative”: [https://defensescoop.com/2025/09/03/dod-replicator-drone-tech-transition-fielding-questions-linger/](https://defensescoop.com/2025/09/03/dod-replicator-drone-tech-transition-fielding-questions-linger/)

## Why building ASI is hard to justify today

1. Misalignment and loss-of-control risk remain unresolved at superhuman scales; best-available analyses keep non-trivial probabilities on catastrophic failure without new guarantees. ([arXiv][9])
   Textual source:

* Carlsmith, “Is Power-Seeking AI an Existential Risk?”: [https://arxiv.org/abs/2206.13353](https://arxiv.org/abs/2206.13353)

2. Systemic risk and misuse scale with capabilities; governments now acknowledge severe downside scenarios but have not yet operationalized binding controls proportionate to ASI risk. ([GOV.UK][10], [AP News][11])
   Textual sources:

* UK Government, “International AI Safety Report 2025”: [https://www.gov.uk/government/publications/international-ai-safety-report-2025](https://www.gov.uk/government/publications/international-ai-safety-report-2025)
* AP, “General-purpose AI could lead to array of new risks…”: [https://apnews.com/article/7b9db4ca69a89a4dd04e05a4294a3dfd](https://apnews.com/article/7b9db4ca69a89a4dd04e05a4294a3dfd)

3. Capability and compute are compounding fast, increasing accident costs and shortening warning times; this trend outpaces governance capacity. ([Epoch AI][12])
   Textual sources:

* Epoch, “Training compute of frontier AI models grows by 4–5× per year”: [https://epoch.ai/blog/training-compute-of-frontier-ai-models-grows-by-4-5x-per-year](https://epoch.ai/blog/training-compute-of-frontier-ai-models-grows-by-4-5x-per-year)
* Epoch, “Machine Learning Trends—Frontier training compute”: [https://epoch.ai/trends](https://epoch.ai/trends)

## A legitimate path to ASI requires passing these “go/no-go” gates

1. Pre-deployment safety cases with external verification: formal, evidence-based “safety cases” that argue and show model safety under credible threat models; independently audited. ([Centre for the Governance of AI][13])
   Textual source:

* GovAI, “Safety Cases for Frontier AI”: [https://www.governance.ai/research-paper/safety-cases-for-frontier-ai](https://www.governance.ai/research-paper/safety-cases-for-frontier-ai)

2. Capability evals and red-team standards targeted at dangerous autonomy, deception, bio/cyber offense, and power-seeking behaviors, with publish-or-pause commitments agreed internationally. ([metr.org][14])
   Textual source:

* METR, “Frontier AI Safety Commitments”: [https://metr.org/faisc](https://metr.org/faisc)

3. Binding international coordination and disclosure: model risk reporting, incident sharing, and threshold-based restrictions codified via treaties or interoperable domestic law. Current signals exist but are early. ([GOV.UK][15], [Digital Strategy][16], [oxfordmartin.ox.ac.uk][17])
   Textual sources:

* UK Government, “Bletchley Declaration”: [https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023](https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023)
* European Commission, “EU AI Act—risk-based framework”: [https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai](https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai)
* Oxford Martin, “International governance through domestic law in the forthcoming UK Frontier AI Bill”: [https://www.oxfordmartin.ox.ac.uk/publications/international-governance-through-domestic-law-in-the-forthcoming-uk-frontier-ai-bill](https://www.oxfordmartin.ox.ac.uk/publications/international-governance-through-domestic-law-in-the-forthcoming-uk-frontier-ai-bill)

4. Compute, data, and model-deployment controls with independent monitoring: auditable logs, hardware attestation, and kill-switches for dangerous emergent capabilities. Trend data justify urgency. ([Epoch AI][12])
   Textual source:

* Epoch, “Training compute … grows by 4–5× per year”: [https://epoch.ai/blog/training-compute-of-frontier-ai-models-grows-by-4-5x-per-year](https://epoch.ai/blog/training-compute-of-frontier-ai-models-grows-by-4-5x-per-year)

5. Proven post-deployment containment: sandboxing, tripwires, and rollback plans validated in live fire exercises against strong adversarial teams. (Consensus need identified in international reports; concrete proofs remain scarce.) ([GOV.UK][10])
   Textual source:

* UK Government, “International AI Safety Report 2025”: [https://www.gov.uk/government/publications/international-ai-safety-report-2025](https://www.gov.uk/government/publications/international-ai-safety-report-2025)

## Decision rule

* If an actor cannot satisfy 1–5 with independent verification and enforceable oversight, then stop at Narrow-AGI. The expected global risk dominates the private benefit. ([GOV.UK][10], [arXiv][9])
  Textual sources:

* UK Government, “International AI Safety Report 2025”: [https://www.gov.uk/government/publications/international-ai-safety-report-2025](https://www.gov.uk/government/publications/international-ai-safety-report-2025)

* Carlsmith (2022), “Is Power-Seeking AI an Existential Risk?”: [https://arxiv.org/abs/2206.13353](https://arxiv.org/abs/2206.13353)

* If an actor can satisfy 1–5, the pursuit can be legitimate because the upside is material in science, defense deterrence, and productivity, and residual risk is bounded by governance. ([Nature][1], [NBER][4])
  Textual sources:

* Nature AlphaFold: [https://www.nature.com/articles/s41586-021-03819-2](https://www.nature.com/articles/s41586-021-03819-2)

* Nature AlphaTensor: [https://www.nature.com/articles/s41586-022-05172-4](https://www.nature.com/articles/s41586-022-05172-4)

* Nature AlphaDev: [https://www.nature.com/articles/s41586-023-06004-9](https://www.nature.com/articles/s41586-023-06004-9)

* NBER w31161: [https://www.nber.org/papers/w31161](https://www.nber.org/papers/w31161)

## Current status snapshot (September 5, 2025)

* Capabilities and compute are accelerating; eval and governance are catching up but not yet sufficient for ASI-class autonomy. Conclusion: Narrow-AGI is justified now; ASI requires stricter international controls than currently exist. ([Epoch AI][12], [GOV.UK][10])
  Textual sources:
* Epoch compute trend: [https://epoch.ai/blog/training-compute-of-frontier-ai-models-grows-by-4-5x-per-year](https://epoch.ai/blog/training-compute-of-frontier-ai-models-grows-by-4-5x-per-year)
* UK International AI Safety Report 2025: [https://www.gov.uk/government/publications/international-ai-safety-report-2025](https://www.gov.uk/government/publications/international-ai-safety-report-2025)
* Bletchley Declaration: [https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023](https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023)

- [The Wall Street Journal](https://www.wsj.com/world/ai-powered-drone-swarms-have-now-entered-the-battlefield-2cab0f05?utm_source=chatgpt.com)
- [AP News](https://apnews.com/article/9e01b9d35ec1541755a273c07dfa4c71?utm_source=chatgpt.com)
- [The Wall Street Journal](https://www.wsj.com/tech/ai/us-military-ai-contract-turbineone-c2a146b1?utm_source=chatgpt.com)
- [DefenseScoop](https://defensescoop.com/2025/09/03/dod-replicator-drone-tech-transition-fielding-questions-linger/?utm_source=chatgpt.com)

[1]: https://www.nature.com/articles/s41586-021-03819-2?utm_source=chatgpt.com "Highly accurate protein structure prediction with AlphaFold - Nature"
[2]: https://pubmed.ncbi.nlm.nih.gov/36198780/?utm_source=chatgpt.com "Discovering faster matrix multiplication algorithms with ..."
[3]: https://deepmind.google/discover/blog/alphadev-discovers-faster-sorting-algorithms/?utm_source=chatgpt.com "AlphaDev discovers faster sorting algorithms"
[4]: https://www.nber.org/papers/w31161?utm_source=chatgpt.com "Generative AI at Work"
[5]: https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-economic-potential-of-generative-ai-the-next-productivity-frontier?utm_source=chatgpt.com "The economic potential of generative AI"
[6]: https://www.wsj.com/world/ai-powered-drone-swarms-have-now-entered-the-battlefield-2cab0f05?utm_source=chatgpt.com "AI-Powered Drone Swarms Have Now Entered the Battlefield"
[7]: https://www.diu.mil/replicator?utm_source=chatgpt.com "The Replicator Initiative"
[8]: https://defensescoop.com/2025/09/03/dod-replicator-drone-tech-transition-fielding-questions-linger/?utm_source=chatgpt.com "DOD touts 'successful transition' for Replicator initiative"
[9]: https://arxiv.org/abs/2206.13353?utm_source=chatgpt.com "[2206.13353] Is Power-Seeking AI an Existential Risk?"
[10]: https://www.gov.uk/government/publications/international-ai-safety-report-2025?utm_source=chatgpt.com "International AI Safety Report 2025"
[11]: https://apnews.com/article/7b9db4ca69a89a4dd04e05a4294a3dfd?utm_source=chatgpt.com "General purpose AI could lead to array of new risks, experts say in report ahead of AI summit"
[12]: https://epoch.ai/blog/training-compute-of-frontier-ai-models-grows-by-4-5x-per-year?utm_source=chatgpt.com "Training Compute of Frontier AI Models Grows by 4-5x per ..."
[13]: https://www.governance.ai/research-paper/safety-cases-for-frontier-ai?utm_source=chatgpt.com "Safety Cases for Frontier AI | GovAI"
[14]: https://metr.org/faisc?utm_source=chatgpt.com "Frontier AI Safety Policies"
[15]: https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023?utm_source=chatgpt.com "The Bletchley Declaration by Countries Attending the AI ..."
[16]: https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai?utm_source=chatgpt.com "AI Act | Shaping Europe's digital future - European Union"
[17]: https://www.oxfordmartin.ox.ac.uk/publications/international-governance-through-domestic-law-in-the-forthcoming-uk-frontier-ai-bill?utm_source=chatgpt.com "International Governance through Domestic Law… | Oxford ..."
